# -
看点视频，看点专业书，随机做点笔记存过来

  
***单变量线性回归***  
线性回归预测值，分类问题预测结果偏向于0或1的二值问题  
回归问题常用模型：平方误差代价函数（假设函数与实际值的均差方）  
线性回归的目标函数：min（代价函数）  
Gradient descent 梯度下降法 **注意对每个参数要同步更新**  
  
线性回归算法：梯度下降法与平方代价函数结合（Batch梯度下降法）  
  
在单变量线性回归中，假设函数的参数有两个，a0和a1，其实质就是那一条可见的直线  
代价函数是一个针对m个点而言的误差平方和再求平均，计算量展开式庞大，表达式是个求和式  
而梯度下降函数是在前值和学习率（步长）一定时，对代价函数对应求偏导，整个更新过程是并行的，  
    也就是说整个过程代价函数的参数都是未更新之前的参数  
       
 求导式因为参数只有a0 a1，当对a0求导时，平方项直接拉下来，但是内部还是累加和  
                        当对a1求导时，a1有参数x(i),故求导式不仅是累加和，还有乘积项参数x(i)  
           
           
***多变量线性回归***  
单变量假设函数：h(x) = a0 + a1x;  
多变量假设函数：h(x) = a0 + a1x + a2x +...+ anx;  
在多变量线性回归中，对于参数的更新，单变量中带有参数的a1项为一般式aj的映射式  

梯度下降法实用技巧  
1.特征缩放(控制影响因素差不多，从而使得梯度下降速度更快)  一般用均值归一化方法，(xi - ave) / （max - min）  
2.学习率的选择 绘制J（0）与迭代次数的函数，判断是否是学习率选择过大  经验：3倍扩大  

特征和多项式回归  
可以对已有特征处理，作为新的特征来计算 多项式回归是对假设函数的更改，可能是二次函数，三次函数，也可能是开方函数  

正规方程 求0的最优值  不经过迭代，直接找到最优解  
对于x列向量，多加一列x0全为1，然后构造X矩阵，已知y矩阵，0矩阵最优解公式为 0 = （X转置·X）逆 ·X转置y  
**对于（X转置·X）可能涉及到的不可逆的问题，先看是否有多余特征，比如倍数关系，有的话就删除，再用伪逆解决**  
  
数据较小时用正规方程比较方便 数据较大时用梯度下降法  分界在10000左右  
  
    
***Logisitic回归***  
